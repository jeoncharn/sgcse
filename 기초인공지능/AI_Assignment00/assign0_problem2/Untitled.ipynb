{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7a948d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadEliminateSignFromPaper:\n",
    "    def __init__(self, file):\n",
    "        # chnaging the way of file I/O is possible\n",
    "        with open(file, 'r', encoding='utf8') as f:\n",
    "            self.paper = f.readlines() # a list that contains paragraph-wise elements. note: each paragraph has different number of sentences\n",
    "            \n",
    "        self.sign_dict = dict() # a dictionary whose key and value are the eliminated sign and the number of that sign, respectively\n",
    "        \n",
    "        # divided by . end or /n keyword\n",
    "        self.sentences = list() # a list that contains sentence-wise elements\n",
    "        # changing data-type or name of variables above is possible\n",
    "        \n",
    "        #########################################################\n",
    "        # complete the code below, following assignment guideline\n",
    "        \n",
    "        self.make_sentences_list()\n",
    "        \n",
    "        #########################################################\n",
    "    \n",
    "    \"\"\"it is impossible to change the name of methods (functions) below\"\"\"\n",
    "    def find_eliminate_sign(self, word):\n",
    "        '''find all signs and eliminate them from given \"word\"\n",
    "           (except the period(.) that makes the End Of Sentence)\n",
    "           and return it\n",
    "        '''\n",
    "\n",
    "        no_sign_word = '' # changing this variable also okay\n",
    "        #########################################################\n",
    "        # complete the code below, following assignment guideline\n",
    "        \n",
    "        for i in range(len(word)):\n",
    "            if word[i].isalnum() == False:\n",
    "                if i == len(word)-1 and word[i] == '.':\n",
    "                    no_sign_word += word[i]\n",
    "            else:\n",
    "                no_sign_word += word[i]\n",
    "        #########################################################\n",
    "        return no_sign_word\n",
    "    \n",
    "    # this function compose self.sentences variable to certain form\n",
    "    def make_sentences_list(self):\n",
    "        for paragraph in self.paper:\n",
    "            para_split = paragraph.split()\n",
    "            eliminated_data = [self.find_eliminate_sign(word) for word in para_split]\n",
    "            sentence = \"\"\n",
    "            for word in eliminated_data:\n",
    "                sentence += word\n",
    "                if word[-1] == '.':\n",
    "                    self.sentences.append(sentence)\n",
    "                    sentence = \"\"\n",
    "                else:\n",
    "                    sentence += \" \"\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_sorted_sign(self):\n",
    "        '''return a list \n",
    "           that contains (eliminated sign, the number of that sign) tuples \n",
    "           and is sorted by the number in descending\n",
    "        '''\n",
    "        #########################################################\n",
    "        # complete the code below, following assignment guideline\n",
    "    \n",
    "        for sents in self.paper:\n",
    "            for word in sents.split():\n",
    "                    for i in range(len(word)):\n",
    "                        if word[i].isalnum() == False:\n",
    "                            if i != len(word)-1 or word[i] != '.':\n",
    "                                if word[i] not in self.sign_dict:\n",
    "                                    self.sign_dict[word[i]] = 1\n",
    "                                else:\n",
    "                                    self.sign_dict[word[i]] += 1\n",
    "               \n",
    "        self.sign_dict = sorted(self.sign_dict.items(), key = lambda item : item[1], reverse = True)\n",
    "        return self.sign_dict\n",
    "        #########################################################\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''return the number of sentences'''\n",
    "        #########################################################\n",
    "        # complete the code below, following assignment guideline\n",
    "        # compose sentences variable        \n",
    "        return len(self.sentences)\n",
    "        #########################################################\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"return a sentence that corresponds to the given index \"idx\" \"\"\"\n",
    "        #########################################################\n",
    "        # complete the code below, following assignment guideline\n",
    "        return self.sentences[idx]\n",
    "        #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a5f24f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "testclass = ReadEliminateSignFromPaper(\"input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9775b38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hellopjeoncharnasfsadf.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testclass.find_eliminate_sign(\"hellop-jeoncharn .asfsadf~.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d9554d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 40), ('-', 16), ('.', 15), ('(', 11), (')', 11), (';', 7), (':', 2), ('â€™', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(testclass.get_sorted_sign())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2dd4b778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['While',\n",
       " 'the',\n",
       " 'Transformer',\n",
       " 'architecture',\n",
       " 'has',\n",
       " 'become',\n",
       " 'the',\n",
       " 'defacto',\n",
       " 'standard',\n",
       " 'for',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'tasks',\n",
       " 'its',\n",
       " 'applications',\n",
       " 'to',\n",
       " 'computer',\n",
       " 'vision',\n",
       " 'remain',\n",
       " 'limited.']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testclass.sentences[1].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "01fd1baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d0c20452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AN IMAGE IS WORTH 16X16 WORDS TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.',\n",
       " 'While the Transformer architecture has become the defacto standard for natural language processing tasks its applications to computer vision remain limited.',\n",
       " 'In vision attention is either applied in conjunction with convolutional networks or used to replace certain components of convolutional networks while keeping their overall structure in place.',\n",
       " 'We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks.',\n",
       " 'When pretrained on large amounts of data and transferred to multiple midsized or small image recognition benchmarks ImageNet CIFAR100 VTAB etc Vision Transformer ViT attains excellent results compared to stateoftheart convolutional networks while requiring substantially fewer computational resources to train.',\n",
       " 'Selfattentionbased architectures in particular Transformers Vaswani et al 2017 have become the model of choice in natural language processing NLP.',\n",
       " 'The dominant approach is to pretrain on a large text corpus and then finetune on a smaller taskspecific dataset Devlin et al 2019.',\n",
       " 'Thanks to Transformers computational efficiency and scalability it has become possible to train models of unprecedented size with over 100B parameters Brown et al 2020 Lepikhin et al 2020.',\n",
       " 'With the models and datasets growing there is still no sign of saturating performance.',\n",
       " 'In computer vision however convolutional architectures remain dominant LeCun et al 1989 Krizhevsky et al 2012 He et al 2016.',\n",
       " 'Inspired by NLP successes multiple works try combining CNNlike architectures with selfattention Wang et al 2018 Carion et al 2020 some replacing the convolutions entirely Ramachandran et al 2019 Wang et al 2020a.',\n",
       " 'The latter models while theoretically efficient have not yet been scaled effectively on modern hardware accelerators due to the use of specialized attention patterns.',\n",
       " 'Therefore in largescale image recognition classic ResNetlike architectures are still state of the art Mahajan et al 2018 Xie et al 2020 Kolesnikov et al 2020.',\n",
       " 'Inspired by the Transformer scaling successes in NLP we experiment with applying a standard Transformer directly to images with the fewest possible modifications.',\n",
       " 'To do so we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer.',\n",
       " 'Image patches are treated the same way as tokens words in an NLP application.',\n",
       " 'We train the model on image classification in supervised fashion.',\n",
       " 'When trained on midsized datasets such as ImageNet without strong regularization these models yield modest accuracies of a few percentage points below ResNets of comparable size.',\n",
       " 'This seemingly discouraging outcome may be expected Transformers lack some of the inductive biases inherent to CNNs such as translation equivariance and locality and therefore do not generalize well when trained on insufficient amounts of data.']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testclass[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f76f94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
