{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63064e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import Hangul\n",
    "\n",
    "# data는 동일한 directory에 위치해 있다.\n",
    "\n",
    "# file에서 data를 받아온다.\n",
    "with open(\"train.txt\", \"r\") as f1:\n",
    "    train_data = f1.readlines()\n",
    "\n",
    "with open(\"test.txt\", \"r\") as f2:\n",
    "    test_data = f2.readlines()\n",
    "\n",
    "# 받은 data를 \\t, \\n (whitespace) 을 기준으로 나눈다.\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i] = train_data[i].split()\n",
    "    \n",
    "for i in range(len(test_data)):\n",
    "    test_data[i] = test_data[i].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34c9b65e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 필요한 dataset 구현\n",
    "cvl_data_set = []\n",
    "for data in train_data:\n",
    "    if data[2] == 'CVL':\n",
    "        if re.search(\"\\w+\", data[1]):\n",
    "            word = re.search(\"\\w+\", data[1]).group()\n",
    "            if word[-1] in \"은는이가을던의를에도들\":\n",
    "                cvl_data_set.append(word[:-1])\n",
    "            else:\n",
    "                cvl_data_set.append(word)\n",
    "                \n",
    "CVL_freqdist = nltk.FreqDist(cvl_data_set)\n",
    "\n",
    "cvl_list = [i for (i, _) in CVL_freqdist.most_common(50)]\n",
    "\n",
    "org_data_set = []\n",
    "for data in train_data:\n",
    "    if data[2] == 'ORG':\n",
    "        if re.search(\"\\w+\", data[1]):\n",
    "            word = re.search(\"\\w+\", data[1]).group()\n",
    "            if word[-1] in \"은는이가을던의를에도들\":\n",
    "                org_data_set.append(word[:-1])\n",
    "            else:\n",
    "                org_data_set.append(word)\n",
    "                \n",
    "ORG_freqdist = nltk.FreqDist(org_data_set)\n",
    "\n",
    "org_list = [i for (i, _) in ORG_freqdist.most_common(50)]\n",
    "org_list\n",
    "\n",
    "trm_data_set = []\n",
    "for data in train_data:\n",
    "    if data[2] == 'TRM':\n",
    "        if re.search(\"\\w+\", data[1]):\n",
    "            word = re.search(\"\\w+\", data[1]).group()\n",
    "            if word[-1] in \"은는이가을던의를에도들\":\n",
    "                trm_data_set.append(word[:-1])\n",
    "            else:\n",
    "                trm_data_set.append(word)\n",
    "                \n",
    "TRM_freqdist = nltk.FreqDist(trm_data_set)\n",
    "\n",
    "trm_list = [i for (i, _) in TRM_freqdist.most_common(50)]\n",
    "\n",
    "loc_data_set = []\n",
    "for data in train_data:\n",
    "    if data[2] == 'LOC':\n",
    "        if re.search(\"\\w+\", data[1]):\n",
    "            word = re.search(\"\\w+\", data[1]).group()\n",
    "            #if word[-1] in \"은는이가을던의를에도들\":\n",
    "            #    trm_data_set.append(word[:-1])\n",
    "            #else:\n",
    "            loc_data_set.append(word)\n",
    "                \n",
    "LOC_freqdist = nltk.FreqDist(loc_data_set)\n",
    "\n",
    "loc_list = [i for (i, _) in LOC_freqdist.most_common(50)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e67cb624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction 영역\n",
    "\n",
    "# 각 문장의 길이와 \n",
    "\n",
    "# 고려할 만한 정보\n",
    "\n",
    "# 맨 첫 문자와, 맨 마지막 문자를 사용할 수 있다.\n",
    "\n",
    "# TIM, DAT 의 경우, 오전, 오후, 전반, 후반, 시, 분, 초 등이 존재할 수 있다.\n",
    "\n",
    "#문자열에 숫자가 존재하는지 (isdigit() = True)을 통해 NUM을 판별해볼 수 있다.\n",
    "\n",
    "\n",
    "# 가장 많이 사용하는 100개의 문자를 토대로 판별한다.\n",
    "# freqdist = nltk.FreqDist(data[1] for data in train_data)\n",
    "# common_list = [i[0] for i in freqdist.most_common(50)]\n",
    "\n",
    "def feature_extraction(dataset, loc):\n",
    "    features = {}\n",
    "    \n",
    "    #시간(DAT, TIM)을 위한 data\n",
    "    for letter in \"년월일시분초전후\":\n",
    "        features[f'has({letter})'] = letter in dataset[loc][1]\n",
    "    \n",
    "    features['word'] = dataset[loc][1]\n",
    "        \n",
    "    features['first_one'] = dataset[loc][1][0]\n",
    "    features['first_two'] = dataset[loc][1][:2] if len(dataset[loc][1]) > 1 else dataset[loc][1]\n",
    "    features['first_three'] = dataset[loc][1][:3] if len(dataset[loc][1]) > 2 else dataset[loc][1]\n",
    "    \n",
    "    features['last_one'] = dataset[loc][1][-1]\n",
    "    features['last_two'] = dataset[loc][1][-2:]\n",
    "    features['last_three'] = dataset[loc][1][-3:]\n",
    "    \n",
    "    features['delete_last_one'] = dataset[loc][1][:-1]\n",
    "    features['delete_last_two'] = dataset[loc][1][:-2]\n",
    "    features['delete_last_three'] = dataset[loc][1][:-3]\n",
    "    \n",
    "    features['before_word'] = '' if dataset[loc][0] == '1' else dataset[loc-1][1]\n",
    "    features['after_word'] = '' if loc == len(dataset)-1 or dataset[loc+1][0] == '1' else dataset[loc+1][1]\n",
    "    \n",
    "    features['has_digit'] = any(letter.isdigit() for letter in dataset[loc][1])\n",
    "    features['has_eng'] = True if re.search('[a-zA-Z]', dataset[loc][1]) else False\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "    #features['length'] = len(data[1])\n",
    "    \n",
    "    \"\"\"\n",
    "    features['in_cvl_list'] = False\n",
    "    for i in cvl_list:\n",
    "        if i in data[1]:\n",
    "            features['in_cvl_list'] = True\n",
    "    \n",
    "    features['in_org_list'] = False\n",
    "    for i in org_list:\n",
    "        if i in data[1]:\n",
    "            features['in_org_list'] = True\n",
    "    \n",
    "    features['in_trm_list'] = False\n",
    "    for i in trm_list:\n",
    "        if i in data[1]:\n",
    "            features['in_trm_list'] = True\n",
    "    \n",
    "    features['in_loc_list'] = False\n",
    "    for i in loc_list:\n",
    "        if i in data[1]:\n",
    "            features['in_loc_list'] = True\n",
    "    \"\"\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a80f661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8702424829349932"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train, test_data의 모든 data에 feature_extraction을 적용하여 train, test_set 구현\n",
    "train_set = [(feature_extraction(train_data, i), train_data[i][2])\n",
    "             for i in range(len(train_data))]\n",
    "test_set = [(feature_extraction(test_data, i), test_data[i][2])\n",
    "             for i in range(len(test_data))]\n",
    "\n",
    "# classifier을 만들고, test_set에 대해 accuracy를 측정한다.\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8cedb48c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  string = '.'                 - : AFW    =   6363.9 : 1.0\n",
      "            first_letter = '1'               NUM : PER    =   4848.0 : 1.0\n",
      "       delete_last_three = '2'               NUM : -      =   4507.3 : 1.0\n",
      "              last_three = '.'                 - : AFW    =   4359.6 : 1.0\n",
      "            first_letter = '2'               NUM : -      =   3917.3 : 1.0\n",
      "       delete_last_three = '1'               NUM : -      =   3830.6 : 1.0\n",
      "             last_letter = 'r'               TRM : -      =   3452.9 : 1.0\n",
      "                last_two = '두'               NUM : -      =   3447.3 : 1.0\n",
      "                last_two = '1일'              DAT : -      =   3335.5 : 1.0\n",
      "            first_letter = '7'               DAT : -      =   3141.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbfce279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error 가 자주 발생하는 어휘 판별\n",
    "errors = []\n",
    "for i in range(len(test_data)):\n",
    "    guess = classifier.classify(feature_extraction(test_data, i))\n",
    "    if guess != test_data[i][2]:\n",
    "        errors.append((guess, test_data[i][2], test_data[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afdb5e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('NUM', '-', '한'): 531, ('DAT', '-', '지난'): 130, ('DAT', '-', '전'): 118, ('NUM', 'DAT', '한'): 109, ('DAT', '-', '호시기'): 76, ('DAT', '-', '계절'): 70, ('DAT', '-', '유행기'): 70, ('DAT', '-', '몇'): 69, ('DAT', '-', '월경기'): 68, ('DAT', '-', '올'): 65, ...})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors\n",
    "freq = nltk.FreqDist([data for data in errors])\n",
    "freq\n",
    "# len(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0516fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdist = nltk.FreqDist(data[2] for data in test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3c85ebb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-', 292164),\n",
       " ('NUM', 25966),\n",
       " ('CVL', 23972),\n",
       " ('PER', 19280),\n",
       " ('ORG', 18310),\n",
       " ('DAT', 13589),\n",
       " ('TRM', 8885),\n",
       " ('LOC', 8370),\n",
       " ('EVT', 7029),\n",
       " ('ANM', 2609),\n",
       " ('AFW', 2451),\n",
       " ('TIM', 1647),\n",
       " ('FLD', 949),\n",
       " ('MAT', 124),\n",
       " ('PLT', 87)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqdist.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52421146",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 0.878\n",
      "NUM 0.916\n",
      "CVL 0.881\n",
      "PER 0.898\n",
      "ORG 0.848\n",
      "DAT 0.92\n",
      "TRM 0.735\n",
      "LOC 0.814\n",
      "EVT 0.752\n",
      "ANM 0.751\n",
      "AFW 0.432\n",
      "TIM 0.862\n",
      "FLD 0.547\n",
      "MAT 0.073\n",
      "PLT 0.138\n"
     ]
    }
   ],
   "source": [
    "for tag, _ in freqdist.most_common():\n",
    "    data_div_set = []\n",
    "    for data in test_set:\n",
    "        if data[1] == tag:\n",
    "            data_div_set.append(data)\n",
    "    \n",
    "    predict = nltk.classify.accuracy(classifier, data_div_set)\n",
    "    print(tag, round(predict, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788606b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수를 바탕으로 cvl dataset 얻어내기\n",
    "cvl_data_set = []\n",
    "for data in train_data:\n",
    "    if data[2] == 'CVL':\n",
    "        if re.search(\"\\w+\", data[1]):\n",
    "            word = re.search(\"\\w+\", data[1]).group()\n",
    "            if word[-1] in \"은는이가을던의를에도들\":\n",
    "                cvl_data_set.append(word[:-1])\n",
    "            else:\n",
    "                cvl_data_set.append(word)\n",
    "                \n",
    "CVL_freqdist = nltk.FreqDist(cvl_data_set)\n",
    "\n",
    "cvl_list = [i for (i, _) in CVL_freqdist.most_common(50)]\n",
    "\n",
    "# feature_extraction 함수 내부에서 아래와 같이 feature로 추가\n",
    "features['in_cvl_list'] = False\n",
    "for i in cvl_list:\n",
    "    if i in dataset[loc][1]:\n",
    "        features['in_cvl_list'] = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
